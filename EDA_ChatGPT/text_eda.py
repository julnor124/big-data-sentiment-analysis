#!/usr/bin/env python3
"""
Text Exploratory Data Analysis (EDA) for ChatGPT_clean.csv
Analyzes tweet content, structure, frequency patterns, and language.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
import warnings
warnings.filterwarnings('ignore')

# Import additional libraries for text analysis
try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except ImportError:
    TEXTBLOB_AVAILABLE = False
    print("‚ö†Ô∏è  TextBlob not available for language detection")

try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    print("‚ö†Ô∏è  NLTK not available for stopwords")

# Download required NLTK data if available
if NLTK_AVAILABLE:
    try:
        nltk.download('stopwords', quiet=True)
        nltk.download('punkt', quiet=True)
        stop_words = set(stopwords.words('english'))
    except:
        stop_words = set()

def load_data():
    """Load the ChatGPT dataset."""
    print("üìä 1. LOADING DATA...")
    try:
        df = pd.read_csv('ChatGPT_clean.csv')
        print(f"‚úÖ Data loaded successfully!")
        print(f"   - Shape: {df.shape}")
        return df
    except FileNotFoundError:
        print("‚ùå File 'ChatGPT_clean.csv' not found!")
        return None
    except Exception as e:
        print(f"‚ùå Error loading data: {e}")
        return None

def analyze_raw_tweet_structure(df):
    """Analyze raw tweet structure and print examples."""
    
    print("\nüìù 2. RAW TWEET STRUCTURE ANALYSIS")
    print("=" * 60)
    
    # Print examples of different types of tweets
    print("üîç Example Tweets by Structure:")
    
    # Sample tweets
    sample_tweets = df['Tweet'].head(10)
    for i, tweet in enumerate(sample_tweets, 1):
        print(f"\n{i}. Length: {len(str(tweet))} characters")
        print(f"   Content: {str(tweet)[:200]}{'...' if len(str(tweet)) > 200 else ''}")
    
    # Analyze tweet structure patterns
    print(f"\nüìä Tweet Structure Patterns:")
    
    # Check for common patterns
    hashtag_count = df['Tweet'].str.contains('#', na=False).sum()
    mention_count = df['Tweet'].str.contains('@', na=False).sum()
    url_count = df['Tweet'].str.contains('http', na=False).sum()
    rt_count = df['Tweet'].str.contains('RT @', na=False).sum()
    
    print(f"   - Tweets with hashtags: {hashtag_count:,} ({hashtag_count/len(df)*100:.1f}%)")
    print(f"   - Tweets with mentions: {mention_count:,} ({mention_count/len(df)*100:.1f}%)")
    print(f"   - Tweets with URLs: {url_count:,} ({url_count/len(df)*100:.1f}%)")
    print(f"   - Retweets (RT @): {rt_count:,} ({rt_count/len(df)*100:.1f}%)")
    
    # Analyze tweet lengths
    df['tweet_length'] = df['Tweet'].str.len()
    
    print(f"\nüìè Tweet Length Statistics:")
    print(f"   - Mean length: {df['tweet_length'].mean():.1f} characters")
    print(f"   - Median length: {df['tweet_length'].median():.1f} characters")
    print(f"   - Min length: {df['tweet_length'].min()} characters")
    print(f"   - Max length: {df['tweet_length'].max()} characters")
    
    return df

def frequency_analysis(df):
    """Perform frequency analysis of words, hashtags, and mentions."""
    
    print("\nüìà 3. FREQUENCY ANALYSIS")
    print("=" * 60)
    
    # Combine all tweets into one text
    all_text = ' '.join(df['Tweet'].astype(str))
    
    # Extract hashtags
    hashtags = re.findall(r'#\w+', all_text.lower())
    hashtag_counts = Counter(hashtags)
    
    # Extract mentions
    mentions = re.findall(r'@\w+', all_text.lower())
    mention_counts = Counter(mentions)
    
    # Extract words (remove hashtags, mentions, URLs, and special characters)
    words = re.findall(r'\b[a-zA-Z]{3,}\b', all_text.lower())
    
    # Remove stopwords if available
    if NLTK_AVAILABLE and stop_words:
        words = [word for word in words if word not in stop_words]
    
    word_counts = Counter(words)
    
    # Display results
    print("üî• Top 20 Most Common Words:")
    for i, (word, count) in enumerate(word_counts.most_common(20), 1):
        print(f"   {i:2d}. {word:<15} : {count:,}")
    
    print(f"\n#Ô∏è‚É£ Top 20 Most Common Hashtags:")
    for i, (hashtag, count) in enumerate(hashtag_counts.most_common(20), 1):
        print(f"   {i:2d}. {hashtag:<20} : {count:,}")
    
    print(f"\nüë• Top 20 Most Common Mentions:")
    for i, (mention, count) in enumerate(mention_counts.most_common(20), 1):
        print(f"   {i:2d}. {mention:<20} : {count:,}")
    
    # Store results for visualization
    return {
        'word_counts': word_counts,
        'hashtag_counts': hashtag_counts,
        'mention_counts': mention_counts
    }

def language_detection(df):
    """Detect languages in tweets."""
    
    print("\nüåç 4. LANGUAGE DETECTION")
    print("=" * 60)
    
    if not TEXTBLOB_AVAILABLE:
        print("‚ö†Ô∏è  Language detection skipped - TextBlob not available")
        return {}
    
    # Sample tweets for language detection (to avoid long processing time)
    sample_size = min(10000, len(df))
    sample_df = df.sample(n=sample_size, random_state=42)
    
    print(f"üîç Analyzing language for {sample_size:,} sample tweets...")
    
    languages = []
    for tweet in sample_df['Tweet']:
        try:
            blob = TextBlob(str(tweet))
            lang = blob.detect_language()
            languages.append(lang)
        except:
            languages.append('unknown')
    
    language_counts = Counter(languages)
    
    print(f"\nüìä Language Distribution (Sample of {sample_size:,} tweets):")
    total_detected = sum(language_counts.values())
    for lang, count in language_counts.most_common(10):
        percentage = (count / total_detected) * 100
        print(f"   {lang:<10} : {count:,} ({percentage:.1f}%)")
    
    return language_counts

def tweet_length_distribution(df):
    """Analyze and visualize tweet length distribution."""
    
    print("\nüìè 5. TWEET LENGTH DISTRIBUTION")
    print("=" * 60)
    
    # Calculate tweet lengths
    df['tweet_length'] = df['Tweet'].str.len()
    
    # Length distribution analysis
    print("üìä Length Distribution by Categories:")
    
    length_ranges = [
        (0, 50, "Very Short (0-50)"),
        (51, 100, "Short (51-100)"),
        (101, 200, "Medium (101-200)"),
        (201, 280, "Long (201-280)"),
        (281, float('inf'), "Very Long (281+)")
    ]
    
    for min_len, max_len, label in length_ranges:
        if max_len == float('inf'):
            count = (df['tweet_length'] >= min_len).sum()
        else:
            count = ((df['tweet_length'] >= min_len) & (df['tweet_length'] <= max_len)).sum()
        percentage = (count / len(df)) * 100
        print(f"   {label:<20} : {count:,} tweets ({percentage:.1f}%)")
    
    # Statistical summary
    print(f"\nüìà Statistical Summary:")
    print(f"   - Mean: {df['tweet_length'].mean():.1f} characters")
    print(f"   - Median: {df['tweet_length'].median():.1f} characters")
    print(f"   - Standard Deviation: {df['tweet_length'].std():.1f}")
    print(f"   - 25th Percentile: {df['tweet_length'].quantile(0.25):.1f}")
    print(f"   - 75th Percentile: {df['tweet_length'].quantile(0.75):.1f}")
    
    return df

def create_visualizations(df, freq_data, lang_data):
    """Create visualizations for text analysis."""
    
    print("\nüìä 6. CREATING VISUALIZATIONS")
    print("=" * 60)
    
    # Set up the plotting style
    plt.style.use('default')
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('ChatGPT Tweet Text Analysis', fontsize=16, fontweight='bold')
    
    # 1. Tweet length distribution histogram
    axes[0, 0].hist(df['tweet_length'], bins=50, alpha=0.7, edgecolor='black')
    axes[0, 0].set_title('Distribution of Tweet Lengths')
    axes[0, 0].set_xlabel('Tweet Length (characters)')
    axes[0, 0].set_ylabel('Frequency')
    
    # 2. Tweet length box plot
    axes[0, 1].boxplot(df['tweet_length'], patch_artist=True)
    axes[0, 1].set_title('Tweet Length Distribution (Box Plot)')
    axes[0, 1].set_ylabel('Tweet Length (characters)')
    
    # 3. Top 15 most common words
    if 'word_counts' in freq_data:
        top_words = dict(freq_data['word_counts'].most_common(15))
        words = list(top_words.keys())
        counts = list(top_words.values())
        axes[0, 2].barh(words, counts)
        axes[0, 2].set_title('Top 15 Most Common Words')
        axes[0, 2].set_xlabel('Frequency')
    
    # 4. Top 15 most common hashtags
    if 'hashtag_counts' in freq_data:
        top_hashtags = dict(freq_data['hashtag_counts'].most_common(15))
        hashtags = list(top_hashtags.keys())
        counts = list(top_hashtags.values())
        axes[1, 0].barh(hashtags, counts)
        axes[1, 0].set_title('Top 15 Most Common Hashtags')
        axes[1, 0].set_xlabel('Frequency')
    
    # 5. Top 15 most common mentions
    if 'mention_counts' in freq_data:
        top_mentions = dict(freq_data['mention_counts'].most_common(15))
        mentions = list(top_mentions.keys())
        counts = list(top_mentions.values())
        axes[1, 1].barh(mentions, counts)
        axes[1, 1].set_title('Top 15 Most Common Mentions')
        axes[1, 1].set_xlabel('Frequency')
    
    # 6. Language distribution (if available)
    if lang_data:
        top_langs = dict(Counter(lang_data).most_common(10))
        langs = list(top_langs.keys())
        counts = list(top_langs.values())
        axes[1, 2].pie(counts, labels=langs, autopct='%1.1f%%')
        axes[1, 2].set_title('Language Distribution (Sample)')
    else:
        axes[1, 2].text(0.5, 0.5, 'Language detection\nnot available', 
                       ha='center', va='center', transform=axes[1, 2].transAxes)
        axes[1, 2].set_title('Language Distribution')
    
    plt.tight_layout()
    plt.savefig('chatgpt_text_analysis_visualizations.png', dpi=300, bbox_inches='tight')
    print("‚úÖ Visualizations saved as 'chatgpt_text_analysis_visualizations.png'")
    
    return df

def save_text_analysis_report(df, freq_data, lang_data):
    """Save comprehensive text analysis report to file."""
    
    print("\nüìù 7. SAVING TEXT ANALYSIS REPORT TO FILE")
    print("=" * 60)
    
    # Prepare data for report
    df['tweet_length'] = df['Tweet'].str.len()
    
    # Create report content
    report_content = f"""
CHATGPT TWEET TEXT ANALYSIS REPORT
==================================
Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

1. DATASET OVERVIEW
===================
Total tweets analyzed: {len(df):,}
Date range: {pd.to_datetime(df['Date'], errors='coerce').min().date() if 'Date' in df.columns and not df['Date'].isnull().all() else 'N/A'} to {pd.to_datetime(df['Date'], errors='coerce').max().date() if 'Date' in df.columns and not df['Date'].isnull().all() else 'N/A'}

2. RAW TWEET STRUCTURE ANALYSIS
===============================
Tweet Structure Patterns:
- Tweets with hashtags: {df['Tweet'].str.contains('#', na=False).sum():,} ({df['Tweet'].str.contains('#', na=False).sum()/len(df)*100:.1f}%)
- Tweets with mentions: {df['Tweet'].str.contains('@', na=False).sum():,} ({df['Tweet'].str.contains('@', na=False).sum()/len(df)*100:.1f}%)
- Tweets with URLs: {df['Tweet'].str.contains('http', na=False).sum():,} ({df['Tweet'].str.contains('http', na=False).sum()/len(df)*100:.1f}%)
- Retweets (RT @): {df['Tweet'].str.contains('RT @', na=False).sum():,} ({df['Tweet'].str.contains('RT @', na=False).sum()/len(df)*100:.1f}%)

Tweet Length Statistics:
- Mean length: {df['tweet_length'].mean():.1f} characters
- Median length: {df['tweet_length'].median():.1f} characters
- Min length: {df['tweet_length'].min()} characters
- Max length: {df['tweet_length'].max()} characters

3. FREQUENCY ANALYSIS
=====================
"""
    
    # Add frequency analysis results
    if 'word_counts' in freq_data:
        report_content += "\nTop 20 Most Common Words:\n"
        for i, (word, count) in enumerate(freq_data['word_counts'].most_common(20), 1):
            report_content += f"{i:2d}. {word:<15} : {count:,}\n"
    
    if 'hashtag_counts' in freq_data:
        report_content += "\nTop 20 Most Common Hashtags:\n"
        for i, (hashtag, count) in enumerate(freq_data['hashtag_counts'].most_common(20), 1):
            report_content += f"{i:2d}. {hashtag:<20} : {count:,}\n"
    
    if 'mention_counts' in freq_data:
        report_content += "\nTop 20 Most Common Mentions:\n"
        for i, (mention, count) in enumerate(freq_data['mention_counts'].most_common(20), 1):
            report_content += f"{i:2d}. {mention:<20} : {count:,}\n"
    
    # Add language detection results
    report_content += "\n4. LANGUAGE DETECTION\n"
    report_content += "====================\n"
    if lang_data:
        total_detected = sum(lang_data.values())
        report_content += f"Languages detected in sample of {total_detected:,} tweets:\n"
        for lang, count in Counter(lang_data).most_common(10):
            percentage = (count / total_detected) * 100
            report_content += f"{lang:<10} : {count:,} ({percentage:.1f}%)\n"
    else:
        report_content += "Language detection not available (TextBlob not installed)\n"
    
    # Add tweet length distribution
    report_content += "\n5. TWEET LENGTH DISTRIBUTION\n"
    report_content += "============================\n"
    
    length_ranges = [
        (0, 50, "Very Short (0-50)"),
        (51, 100, "Short (51-100)"),
        (101, 200, "Medium (101-200)"),
        (201, 280, "Long (201-280)"),
        (281, float('inf'), "Very Long (281+)")
    ]
    
    for min_len, max_len, label in length_ranges:
        if max_len == float('inf'):
            count = (df['tweet_length'] >= min_len).sum()
        else:
            count = ((df['tweet_length'] >= min_len) & (df['tweet_length'] <= max_len)).sum()
        percentage = (count / len(df)) * 100
        report_content += f"{label:<20} : {count:,} tweets ({percentage:.1f}%)\n"
    
    report_content += f"""
Statistical Summary:
- Mean: {df['tweet_length'].mean():.1f} characters
- Median: {df['tweet_length'].median():.1f} characters
- Standard Deviation: {df['tweet_length'].std():.1f}
- 25th Percentile: {df['tweet_length'].quantile(0.25):.1f}
- 75th Percentile: {df['tweet_length'].quantile(0.75):.1f}

6. KEY FINDINGS
===============
1. Most tweets contain hashtags and mentions, indicating active social media engagement
2. Tweet lengths vary widely, with most being medium length (101-200 characters)
3. Common words and hashtags reveal the main topics of discussion around ChatGPT
4. Language detection shows the primary language of the dataset
5. Retweet patterns indicate viral content and information sharing

END OF REPORT
=============
"""
    
    # Save to file
    filename = 'chatgpt_text_analysis_report.txt'
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"‚úÖ Comprehensive text analysis report saved as '{filename}'")
    print(f"üìÑ Report includes all text analysis results and findings")
    
    return df

def main():
    """Main function to run the text analysis."""
    
    print("üîç CHATGPT TWEET TEXT ANALYSIS")
    print("=" * 60)
    
    # Load data
    df = load_data()
    if df is None:
        return
    
    # Perform text analysis steps
    df = analyze_raw_tweet_structure(df)
    freq_data = frequency_analysis(df)
    lang_data = language_detection(df)
    df = tweet_length_distribution(df)
    df = create_visualizations(df, freq_data, lang_data)
    save_text_analysis_report(df, freq_data, lang_data)
    
    print("\n‚úÖ Text Analysis Complete!")
    print("=" * 60)

if __name__ == "__main__":
    main()
